{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixup Initialization: Residual Learning Without Normalization\n",
    "In this notebook we will attempt to explain and replicate findings from: https://arxiv.org/pdf/1901.09321.pdf\n",
    "https://github.com/hongyi-zhang/Fixup/blob/master/cifar/models/fixup_resnet_cifar.py\n",
    "\n",
    "We will also model $N$ layered Neural Networks with these techniques and measure the affect on the **Activation & Weight** Statistics. \n",
    "\n",
    "This will give us a clear idea on how it will effect learning.\n",
    "\n",
    "## Abstract\n",
    "Normalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabalize training, enabling higher learning rate, accelerate convergence and imrpove generalization, though the reason for their effectiveness is still an active research topic. \n",
    "\n",
    "## Introduction\n",
    "Artificial Intelligence applications have witnessed major advances in recent years. At the core of this revolution is the development of novel neural network models and their training techniques. For example, since the landmark work of He. Most of the state-of-the-art image recognition systems are built upon a deep stack of network blocks consisting of convolutional layers and additive skip connections, with some normalization mehcanisms to facilitate training and generalization. \n",
    "\n",
    "Various normalization techniques have been found essential to achieving good performance on other tasks, such as machine translation and generative modeling. They are widely believed to have multiple benefits for training very deep neural networks, including stabalizing learning, enabling higher learning rate, accelerating convergence, and improving generalization. \n",
    "\n",
    "### Problem: ResNet with standard initialization lead to exploding gradients\n",
    "Standard initialization methods attempt to set the initial parameters of the network such that the activations neither vanish nor explode. Unfortionately, it has been observed that without normalization techniques such as BatchNorm they do not account properly for the effect of residual connections and this causes exploding gradients. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
